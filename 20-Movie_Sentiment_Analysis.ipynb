{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fbd36bd",
   "metadata": {},
   "source": [
    "# Applying Machine Learning to Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962e8e39",
   "metadata": {},
   "source": [
    "The topics that we will cover include the following:\n",
    "    - Cleaning and processing text data\n",
    "    - Building feature vector from text documents\n",
    "    - Trainng a machine learning model to classify positive and negative movie reviews\n",
    "\n",
    "## Obtaining the movie review dataset\n",
    "\n",
    "We can download a compressed archive of the movie review dataset(80.2 MB) from\n",
    "[https://ai.stanford.edu/~amaas/data/sentiment/] as Gzip-compressed tarball archive.\n",
    "\n",
    "To Extract data from Gzip archive you can use 7-zip application. You may need to istall it from [http://www.7-zip.org]\n",
    "\n",
    "Alternatively, you can directly unpack the Gzip-compressed tarball archive directly in Python as follows:\n",
    "\n",
    "```\n",
    "import tarfile\n",
    "with tarfile.open('aclImdb_v1.tar.gz', 'r:gz') as tar:\n",
    "    tar.extractall()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a33cce29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load labraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os \n",
    "import shap # labirary for SHAP analysis\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Display all the columns of the dataframe\n",
    "pd.pandas.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67de5804",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "target = 'aclImdb_v1.tar.gz'\n",
    "\n",
    "with tarfile.open(target, 'r:gz') as tar:\n",
    "    tar.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91ecccb",
   "metadata": {},
   "source": [
    "### Preprocessing the movie dataset into more convenient format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6120a5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install pyprind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135b7255",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyprind\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# change the `basepath` to the directory of the\n",
    "# unzipped movie dataset\n",
    "\n",
    "basepath = 'aclImdb'\n",
    "\n",
    "labels = {'pos': 1, 'neg': 0}\n",
    "pbar = pyprind.ProgBar(50000)\n",
    "df = pd.DataFrame()\n",
    "for s in ('test', 'train'):\n",
    "    for l in ('pos', 'neg'):\n",
    "        path = os.path.join(basepath, s, l)\n",
    "        for file in sorted(os.listdir(path)):\n",
    "            with open(os.path.join(path, file), \n",
    "                      'r', encoding='utf-8') as infile:\n",
    "                txt = infile.read()\n",
    "            df = df.append([[txt, labels[l]]], \n",
    "                           ignore_index=True)\n",
    "            pbar.update()\n",
    "df.columns = ['review', 'sentiment']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9f30ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Shuffling the DataFrame:\n",
    "np.random.seed(0)\n",
    "df = df.reindex(np.random.permutation(df.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed65cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eddb806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Saving the assembled data as CSV file:\n",
    "df.to_csv('movie_data.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf327343",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In 1974, the teenager Martha Moxley (Maggie Gr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OK... so... I really like Kris Kristofferson a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>***SPOILER*** Do not read this, if you think a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hi for all the people who have seen this wonde...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I recently bought the DVD, forgetting just how...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  In 1974, the teenager Martha Moxley (Maggie Gr...          1\n",
       "1  OK... so... I really like Kris Kristofferson a...          0\n",
       "2  ***SPOILER*** Do not read this, if you think a...          0\n",
       "3  hi for all the people who have seen this wonde...          1\n",
       "4  I recently bought the DVD, forgetting just how...          0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read from file into DataFrame:\n",
    "df = pd.read_csv('movie_data.csv', encoding='utf-8')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24713898",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data Frame Shape:\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "578dc45b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    25000\n",
       "1    25000\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# New Count HER2 Positive and Negative after resolving Unknown problem\n",
    "df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefed747",
   "metadata": {},
   "source": [
    "## Introducing the bag-of-words model\n",
    "\n",
    "### Transforming documents into feature vectors\n",
    "\n",
    "By calling the fit_transform method on CountVectorizer, we just constructed the vocabulary of the bag-of-words model and transformed the following three sentences into sparse feature vectors:\n",
    "\n",
    "1. The sun is shining\n",
    "2. The weather is sweet\n",
    "3. The sun is shining, the weather is sweet, and one and one is two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b438a6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count = CountVectorizer()\n",
    "docs = np.array([\n",
    "        'The sun is shining',\n",
    "        'The weather is sweet',\n",
    "        'The sun is shining, the weather is sweet, and one and one is two'])\n",
    "bag = count.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a6550f",
   "metadata": {},
   "source": [
    "Now let us print the contents of the vocabulary to get a better understanding of the underlying concepts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7970dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 6, 'sun': 4, 'is': 1, 'shining': 3, 'weather': 8, 'sweet': 5, 'and': 0, 'one': 2, 'two': 7}\n"
     ]
    }
   ],
   "source": [
    "print(count.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad420ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and 0\n",
      "is 1\n",
      "one 2\n",
      "shining 3\n",
      "sun 4\n",
      "sweet 5\n",
      "the 6\n",
      "two 7\n",
      "weather 8\n"
     ]
    }
   ],
   "source": [
    "for k,v in sorted(count.vocabulary_.items()):\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55fc82f",
   "metadata": {},
   "source": [
    "As we can see from executing the preceding command, the vocabulary is stored in a Python dictionary, which maps the unique words that are mapped to integer indices. Next let us print the feature vectors that we just created:\n",
    "\n",
    "Each index position in the feature vectors shown here corresponds to the integer values that are stored as dictionary items in the CountVectorizer vocabulary. For example, the rst feature at index position 0 resembles the count of the word and, which only occurs in the last document, and the word is at index position 1 (the 2nd feature in the document vectors) occurs in all three sentences. Those values in the feature vectors are also called the raw term frequencies: tf *(t,d)* — the number of times a term t occurs in a document *d*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ff71b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'The sun is shining',\n",
    "# Vocab   Val  VocabPosition\n",
    "# and      0      1\n",
    "# is       1      2\n",
    "# one      0      3\n",
    "# shining  1      4\n",
    "# sun      1      5\n",
    "# sweet    0      6\n",
    "# the      1      7\n",
    "# two      0      8\n",
    "# weather  0      9\n",
    "\n",
    "# 'The weather is sweet',\n",
    "# 'The sun is shining, the weather is sweet, and one and one is two'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8577b87c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 1 1 0 1 0 0]\n",
      " [0 1 0 0 0 1 1 0 1]\n",
      " [2 3 2 1 1 1 2 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(bag.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d19277",
   "metadata": {},
   "source": [
    "### Assessing word relevancy via term frequency-inverse document frequency\n",
    "\n",
    "When we are analyzing text data, we often encounter words that occur across multiple documents from both classes. Those frequently occurring words typically don't contain useful or discriminatory information. In this subsection, we will learn about a useful technique called term frequency-inverse document frequency (tf-idf) that can be used to downweight those frequently occurring words in the feature vectors. The tf-idf can be de considered as the product of the term frequency and the inverse document frequency:\n",
    "\n",
    "$$\\text{tf-idf}(t,d)=\\text{tf (t,d)}\\times \\text{idf}(t,d)$$\n",
    "Here the tf(t, d) is the term frequency that we introduced in the previous section, and the inverse document frequency idf(t, d) can be calculated as:\n",
    "\n",
    "$$\\text{idf}(t,d) = \\text{log}\\frac{n_d}{1+\\text{df}(d, t)},$$\n",
    "where $n_d$ is the total number of documents, and df(d, t) is the number of documents d that contain the term t. Note that adding the constant 1 to the denominator is optional and serves the purpose of assigning a non-zero value to terms that occur in all training samples; the log is used to ensure that low document frequencies are not given too much weight.\n",
    "\n",
    "Scikit-learn implements yet another transformer, the TfidfTransformer, that takes the raw term frequencies from CountVectorizer as input and transforms them into tf-idfs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5442d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.   0.43 0.   0.56 0.56 0.   0.43 0.   0.  ]\n",
      " [0.   0.43 0.   0.   0.   0.56 0.43 0.   0.56]\n",
      " [0.5  0.45 0.5  0.19 0.19 0.19 0.3  0.25 0.19]]\n"
     ]
    }
   ],
   "source": [
    "# Set precision\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "# instanciate TfidfTransformer object\n",
    "tfidf = TfidfTransformer(use_idf=True, \n",
    "                         norm='l2', \n",
    "                         smooth_idf=True)\n",
    "# apply TfidfTransformer to our list of documents\n",
    "print(tfidf.fit_transform(count.fit_transform(docs)).toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9bf084",
   "metadata": {},
   "source": [
    "As we saw in the previous subsection, the word **is** had the largest term frequency in the 3rd document, being the most frequently occurring word. However, after transforming the same feature vector into *tf-idfs*, we see that the word is is now associated with a relatively small tf-idf (0.45) in document 3 since it is also contained in documents 1 and 2 and thus is unlikely to contain any useful, discriminatory information.\n",
    "\n",
    "However, if we'd manually calculated the tf-idfs of the individual terms in our feature vectors, we'd have noticed that the TfidfTransformer calculates the tf-idfs slightly differently compared to the standard textbook equations that we saw earlier. The equations for the idf and tf-idf that were implemented in scikit-learn are:\n",
    "\n",
    "$$\\text{idf} (t,d) = log\\frac{1 + n_d}{1 + \\text{df}(d, t)}$$\n",
    "The tf-idf equation that was implemented in scikit-learn is as follows:\n",
    "\n",
    "$$\\text{tf-idf}(t,d) = \\text{tf}(t,d) \\times (\\text{idf}(t,d)+1)$$\n",
    "While it is also more typical to normalize the raw term frequencies before calculating the tf-idfs, the TfidfTransformer normalizes the tf-idfs directly.\n",
    "\n",
    "By default (norm='l2'), scikit-learn's TfidfTransformer applies the L2-normalization, which returns a vector of length 1 by dividing an un-normalized feature vector v by its L2-norm:\n",
    "\n",
    "$$v_{\\text{norm}} = \\frac{v}{||v||_2} = \\frac{v}{\\sqrt{v_{1}^{2} + v_{2}^{2} + \\dots + v_{n}^{2}}} = \\frac{v}{\\big (\\sum_{i=1}^{n} v_{i}^{2}\\big)^\\frac{1}{2}}$$\n",
    "To make sure that we understand how TfidfTransformer works, let us walk through an example and calculate the tf-idf of the word is in the 3rd document.\n",
    "\n",
    "The word **is** has a term frequency of 3 (tf = 3) in document 3, and the document frequency of this term is 3 since the term is occurs in all three documents (df = 3). Thus, we can calculate the idf as follows:\n",
    "\n",
    "$$\\text{idf}(\"is\", d3) = log \\frac{1+3}{1+3} = 0$$\n",
    "Now in order to calculate the tf-idf, we simply need to add 1 to the inverse document frequency and multiply it by the term frequency:\n",
    "\n",
    "$$\\text{tf-idf}(\"is\",d3)= 3 \\times (0+1) = 3$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c839117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf-idf of term \"is\" = 3.00\n"
     ]
    }
   ],
   "source": [
    "# Example of calcualtion TF-IDF for word \"is\" using Python implementation\n",
    "tf_is = 3     # TF- term frequency; number of times that \"is\" in the 3rd sentance\n",
    "n_docs = 3    # number of documents that \"is\" apears in \n",
    "idf_is = np.log((n_docs+1) / (3+1))        # calculationg IDF for \"is\" \n",
    "tfidf_is = tf_is * (idf_is + 1)            # claculating TF-IDF for \"is\"         \n",
    "print('tf-idf of term \"is\" = %.2f' % tfidf_is)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1211b068",
   "metadata": {},
   "source": [
    "If we repeated these calculations for all terms in the 3rd document, we'd obtain the following tf-idf vectors: [3.39, 3.0, 3.39, 1.29, 1.29, 1.29, 2.0 , 1.69, 1.29]. However, we notice that the values in this feature vector are different from the values that we obtained from the TfidfTransformer that we used previously. The nal step that we are missing in this tf-idf calculation is the L2-normalization, which can be applied as follows:\n",
    "\n",
    "\n",
    "$$\\text{tfi-df}_{norm} = \\frac{[3.39, 3.0, 3.39, 1.29, 1.29, 1.29, 2.0 , 1.69, 1.29]}{\\sqrt{[3.39^2, 3.0^2, 3.39^2, 1.29^2, 1.29^2, 1.29^2, 2.0^2 , 1.69^2, 1.29^2]}}$$$$=[0.5, 0.45, 0.5, 0.19, 0.19, 0.19, 0.3, 0.25, 0.19]$$$$\\Rightarrow \\text{tfi-df}_{norm}(\"is\", d3) = 0.45$$\n",
    "\n",
    "\n",
    "As we can see, the results match the results returned by scikit-learn's TfidfTransformer (below). Since we now understand how tf-idfs are calculated, let us proceed to the next sections and apply those concepts to the movie review dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3f354df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.39, 3.  , 3.39, 1.29, 1.29, 1.29, 2.  , 1.69, 1.29])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate TF_IDF for 3rd sentance\n",
    "tfidf = TfidfTransformer(use_idf=True, norm=None, smooth_idf=True)\n",
    "raw_tfidf = tfidf.fit_transform(count.fit_transform(docs)).toarray()[-1]\n",
    "raw_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de7624ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5 , 0.45, 0.5 , 0.19, 0.19, 0.19, 0.3 , 0.25, 0.19])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply L2 norm - normilized TF-IDF vector \n",
    "l2_tfidf = raw_tfidf / np.sqrt(np.sum(raw_tfidf**2))\n",
    "l2_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a5cf6d",
   "metadata": {},
   "source": [
    "## Cleaning text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac3711a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'is seven.<br /><br />Title (Brazil): Not Available'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at last 50 characters of the first review\n",
    "# we would see HTML notations there\n",
    "df.loc[0, 'review'][-50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26a665fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Create preprocessor function to eliminate HTML tags and other characters\n",
    "def preprocessor(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "    text = (re.sub('[\\W]+', ' ', text.lower()) +\n",
    "            ' '.join(emoticons).replace('-', ''))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7394aa88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'is seven title brazil not available'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Applying preprocessor function\n",
    "preprocessor(df.loc[0, 'review'][-50:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7d1e222c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is a test :) :( :)'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Processing emoji - example\n",
    "preprocessor(\"</a>This :) is :( a test :-)!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0ebe0ec2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In 1974, the teenager Martha Moxley (Maggie Grace) moves to the high-class area of Belle Haven, Greenwich, Connecticut. On the Mischief Night, eve of Halloween, she was murdered in the backyard of her house and her murder remained unsolved. Twenty-two years later, the writer Mark Fuhrman (Christopher Meloni), who is a former LA detective that has fallen in disgrace for perjury in O.J. Simpson trial and moved to Idaho, decides to investigate the case with his partner Stephen Weeks (Andrew Mitchell) with the purpose of writing a book. The locals squirm and do not welcome them, but with the support of the retired detective Steve Carroll (Robert Forster) that was in charge of the investigation in the 70\\'s, they discover the criminal and a net of power and money to cover the murder.<br /><br />\"Murder in Greenwich\" is a good TV movie, with the true story of a murder of a fifteen years old girl that was committed by a wealthy teenager whose mother was a Kennedy. The powerful and rich family used their influence to cover the murder for more than twenty years. However, a snoopy detective and convicted perjurer in disgrace was able to disclose how the hideous crime was committed. The screenplay shows the investigation of Mark and the last days of Martha in parallel, but there is a lack of the emotion in the dramatization. My vote is seven.<br /><br />Title (Brazil): Not Available'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at the entire first review before processing\n",
    "df.iloc[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7243f376",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'in 1974 the teenager martha moxley maggie grace moves to the high class area of belle haven greenwich connecticut on the mischief night eve of halloween she was murdered in the backyard of her house and her murder remained unsolved twenty two years later the writer mark fuhrman christopher meloni who is a former la detective that has fallen in disgrace for perjury in o j simpson trial and moved to idaho decides to investigate the case with his partner stephen weeks andrew mitchell with the purpose of writing a book the locals squirm and do not welcome them but with the support of the retired detective steve carroll robert forster that was in charge of the investigation in the 70 s they discover the criminal and a net of power and money to cover the murder murder in greenwich is a good tv movie with the true story of a murder of a fifteen years old girl that was committed by a wealthy teenager whose mother was a kennedy the powerful and rich family used their influence to cover the murder for more than twenty years however a snoopy detective and convicted perjurer in disgrace was able to disclose how the hideous crime was committed the screenplay shows the investigation of mark and the last days of martha in parallel but there is a lack of the emotion in the dramatization my vote is seven title brazil not available'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply preprocessing to our movie reviews\n",
    "df['review'] = df['review'].apply(preprocessor)\n",
    "\n",
    "# See the first review after preprocessing\n",
    "df.iloc[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2236c7a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ok so i really like kris kristofferson and his usual easy going delivery of lines in his movies age has helped him with his soft spoken low energy style and he will steal a scene effortlessly but disappearance is his misstep holy moly this was a bad movie i must give kudos to the cinematography and and the actors including kris for trying their darndest to make sense from this goofy confusing story none of it made sense and kris probably didn t understand it either and he was just going through the motions hoping someone would come up to him and tell him what it was all about i don t care that everyone on this movie was doing out of love for the project or some such nonsense i ve seen low budget movies that had a plot for goodness sake this had none zilcho nada zippo empty of reason a complete waste of good talent scenery and celluloid i rented this piece of garbage for a buck and i want my money back i want my 2 hours back i invested on this grade f waste of my time don t watch this movie or waste 1 minute of your valuable time while passing through a room where it s playing or even open up the case that is holding the dvd believe me you ll thank me for the advice '"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[1, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21368076",
   "metadata": {},
   "source": [
    "## Processing documents into tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1a5bb1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Library for Stemmering \n",
    "# Stemmers remove morphological affixes from words, leaving only the word stem.\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# get PorterStemmer() object\n",
    "porter = PorterStemmer()\n",
    "\n",
    "# tokenization your text - split sentance into seperate words\n",
    "def tokenizer(text):\n",
    "    return text.split()\n",
    "\n",
    "# Tokenizing your words\n",
    "def tokenizer_porter(text):\n",
    "    return [porter.stem(word) for word in text.split()]\n",
    "\n",
    "#from nltk.stem import WordNetLemmatizer\n",
    "#lemmatizer = WordNetLemmatizer()\n",
    "#data[‘pre_process’]=data[‘pre_process’].apply(lambda x: “ “.join([lemmatizer.lemmatize(w) for w in nltk.word_tokenize(x)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1d1f3457",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['runners', 'like', 'running', 'and', 'thus', 'they', 'run']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenizing example\n",
    "tokenizer('runners like running and thus they run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f2ed5544",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['runner', 'like', 'run', 'and', 'thu', 'they', 'run']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenizing and Stemming example\n",
    "tokenizer_porter('runners like running and thus they run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d2e5e123",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\4471082\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Doqnload stop-words\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d00c5f5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['runner', 'like', 'run', 'run', 'lot']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Get the list of stop words\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "# Example of removing stop words from the list \n",
    "[w for w in tokenizer_porter('a runner likes running and runs a lot')[-10:] if w not in stop]\n",
    "\n",
    "#from nltk.corpus import stopwords\n",
    "#stop = stopwords.words(‘english’)\n",
    "#data[‘pre_process’]=data[‘pre_process’].apply(lambda x: “ “.join([x for x in x.split() if x not in stop]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4a3beb5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of English stop-words\n",
    "stop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484ffd3e",
   "metadata": {},
   "source": [
    "## Training a logistic regression model for document classification\n",
    "\n",
    "Strip HTML and punctuation to speed up the GridSearch later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "832e6677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split our review dataset into train and test sets\n",
    "X_train = df.loc[:25000, 'review'].values\n",
    "y_train = df.loc[:25000, 'sentiment'].values\n",
    "X_test = df.loc[25000:, 'review'].values\n",
    "y_test = df.loc[25000:, 'sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f239a7af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF Vectorizer……\n"
     ]
    }
   ],
   "source": [
    "print(\"TFIDF Vectorizer……\")\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer= TfidfVectorizer()\n",
    "tf_x_train = vectorizer.fit_transform(X_train)\n",
    "tf_x_test = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "979c2ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "592aae9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=1000)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(tf_x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "98980cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred=clf.predict(tf_x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "31f50348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.89      0.89     12527\n",
      "           1       0.89      0.90      0.89     12473\n",
      "\n",
      "    accuracy                           0.89     25000\n",
      "   macro avg       0.89      0.89      0.89     25000\n",
      "weighted avg       0.89      0.89      0.89     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "caadf4ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "{'C': 10.0}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.89      0.90     12527\n",
      "           1       0.89      0.90      0.90     12473\n",
      "\n",
      "    accuracy                           0.90     25000\n",
      "   macro avg       0.90      0.90      0.90     25000\n",
      "weighted avg       0.90      0.90      0.90     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# defining parameter range \n",
    "param_grid = {'C': [1.0, 5.0, 10.0, 50.0, 100.0]}\n",
    "\n",
    "grid = GridSearchCV(LogisticRegression(max_iter=1000),\n",
    "                    scoring='accuracy',\n",
    "                    cv=5,\n",
    "                    param_grid = param_grid,\n",
    "                    refit = True,\n",
    "                    verbose = 3,\n",
    "                    n_jobs=-1) \n",
    "   \n",
    "# fitting the model for grid search \n",
    "grid.fit(tf_x_train, y_train) \n",
    " \n",
    "# print best parameter after tuning \n",
    "print(grid.best_params_) \n",
    "\n",
    "grid_predictions = grid.predict(tf_x_test) \n",
    "   \n",
    "# print classification report \n",
    "print(classification_report(y_test, grid_predictions)) \n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4711ba12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "63b04b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library for Logistic regression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "tfidf = TfidfVectorizer(strip_accents=None,\n",
    "                        lowercase=False,\n",
    "                        preprocessor=None)\n",
    "\n",
    "# Parameters for GridSearch\n",
    "param_grid = [{'vect__ngram_range': [(1, 1)],\n",
    "               'vect__stop_words': [stop, None],\n",
    "               'vect__tokenizer': [tokenizer, tokenizer_porter],\n",
    "               'clf__penalty': ['l1', 'l2'],\n",
    "               'clf__C': [1.0, 10.0, 100.0]},\n",
    "              {'vect__ngram_range': [(1, 1)],\n",
    "               'vect__stop_words': [stop, None],\n",
    "               'vect__tokenizer': [tokenizer, tokenizer_porter],\n",
    "               'vect__use_idf':[False],\n",
    "               'vect__norm':[None],\n",
    "               'clf__penalty': ['l1', 'l2'],\n",
    "               'clf__C': [1.0, 10.0, 100.0]},\n",
    "              ]\n",
    "# Creating Pipeline with Logistic Regression Model \n",
    "lr_tfidf = Pipeline([('vect', tfidf),\n",
    "                     ('clf', LogisticRegression(random_state=0))])\n",
    "\n",
    "# Search for bet Logisting Regression Model\n",
    "gs_lr_tfidf = GridSearchCV(lr_tfidf, param_grid,\n",
    "                           scoring='accuracy',\n",
    "                           cv=5,\n",
    "                           verbose=1,\n",
    "                           n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a732d14",
   "metadata": {},
   "source": [
    "**Important Note about n_jobs**\n",
    "\n",
    "Please note that it is highly recommended to use `n_jobs=-1` (instead of `n_jobs=1`) in the previous code example to utilize all available cores on your machine and speed up the grid search. However, some Windows users reported issues when running the previous code with the `n_jobs=-1` setting related to pickling the tokenizer and tokenizer_porter functions for multiprocessing on Windows. Another workaround would be to replace those two functions, `[tokenizer, tokenizer_porter]`, with `[str.split]`. However, note that the replacement by the simple str.split would not support stemming.\n",
    "\n",
    "**Important Note about the running time**\n",
    "\n",
    "Executing the following code cell **may take up to 30-60 min** depending on your machine, since based on the parameter grid we defined, there are 22235 + 22235 = 240 models to fit.\n",
    "\n",
    "If you do not wish to wait so long, you could reduce the size of the dataset by decreasing the number of training samples, for example, as follows:\n",
    "\n",
    "`\n",
    "X_train = df.loc[:2500, 'review'].values\n",
    "y_train = df.loc[:2500, 'sentiment'].values\n",
    "`\n",
    "\n",
    "However, note that decreasing the training set size to such a small number will likely result in poorly performing models. Alternatively, you can delete parameters from the grid above to reduce the number of models to fit -- for example, by using the following:\n",
    "\n",
    "`\n",
    "param_grid = [{'vect__ngram_range': [(1, 1)],\n",
    "               'vect__stop_words': [stop, None],\n",
    "               'vect__tokenizer': [tokenizer],\n",
    "               'clf__penalty': ['l1', 'l2'],\n",
    "               'clf__C': [1.0, 10.0]},\n",
    "              ]\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8b34cf14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "One or more of the test scores are non-finite: [ nan  nan  nan  nan 0.89 0.89 0.89 0.89  nan  nan  nan  nan 0.9  0.89\n",
      " 0.9  0.89  nan  nan  nan  nan 0.89 0.88 0.89 0.88  nan  nan  nan  nan\n",
      " 0.88 0.87 0.88 0.88  nan  nan  nan  nan 0.87 0.86 0.88 0.87  nan  nan\n",
      "  nan  nan 0.87 0.86 0.88 0.87]\n",
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('vect',\n",
       "                                        TfidfVectorizer(lowercase=False)),\n",
       "                                       ('clf',\n",
       "                                        LogisticRegression(random_state=0))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid=[{'clf__C': [1.0, 10.0, 100.0],\n",
       "                          'clf__penalty': ['l1', 'l2'],\n",
       "                          'vect__ngram_range': [(1, 1)],\n",
       "                          'vect__stop_words': [['i', 'me', 'my', 'myself', 'we',\n",
       "                                                'our', 'ours', 'ourselves',\n",
       "                                                'you', \"you're\", \"you've\",\n",
       "                                                \"you'll\", \"you'd\",...\n",
       "                                                'our', 'ours', 'ourselves',\n",
       "                                                'you', \"you're\", \"you've\",\n",
       "                                                \"you'll\", \"you'd\", 'your',\n",
       "                                                'yours', 'yourself',\n",
       "                                                'yourselves', 'he', 'him',\n",
       "                                                'his', 'himself', 'she',\n",
       "                                                \"she's\", 'her', 'hers',\n",
       "                                                'herself', 'it', \"it's\", 'its',\n",
       "                                                'itself', ...],\n",
       "                                               None],\n",
       "                          'vect__tokenizer': [<function tokenizer at 0x000002864D498310>,\n",
       "                                              <function tokenizer_porter at 0x000002864D498AF0>],\n",
       "                          'vect__use_idf': [False]}],\n",
       "             scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train Model \n",
    "gs_lr_tfidf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1e9146e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter set: {'clf__C': 10.0, 'clf__penalty': 'l2', 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__tokenizer': <function tokenizer at 0x000002864D498310>} \n",
      "CV Accuracy: 0.897\n"
     ]
    }
   ],
   "source": [
    "# Print best model parameters\n",
    "print('Best parameter set: %s ' % gs_lr_tfidf.best_params_)\n",
    "\n",
    "# Print Cross-Validation accuracy \n",
    "print('CV Accuracy: %.3f' % gs_lr_tfidf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "df4ab512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.899\n"
     ]
    }
   ],
   "source": [
    "clf = gs_lr_tfidf.best_estimator_\n",
    "print('Test Accuracy: %.3f' % clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f82614ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vect',\n",
       "                 TfidfVectorizer(lowercase=False,\n",
       "                                 tokenizer=<function tokenizer at 0x000002864D498310>)),\n",
       "                ('clf', LogisticRegression(C=10.0, random_state=0))])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_lr_tfidf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b645a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b75f3c49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>in 1974 the teenager martha moxley maggie grac...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ok so i really like kris kristofferson and his...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spoiler do not read this if you think about w...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hi for all the people who have seen this wonde...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i recently bought the dvd forgetting just how ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  in 1974 the teenager martha moxley maggie grac...          1\n",
       "1  ok so i really like kris kristofferson and his...          0\n",
       "2   spoiler do not read this if you think about w...          0\n",
       "3  hi for all the people who have seen this wonde...          1\n",
       "4  i recently bought the dvd forgetting just how ...          0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfn = df.copy()\n",
    "dfn.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8d34c169",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)',\n",
    "                           text.lower())\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) \\\n",
    "                   + ' '.join(emoticons).replace('-', '')\n",
    "    tokenized = [w for w in text.split() if w not in stop]\n",
    "    return tokenized\n",
    "\n",
    "# Example of removing stop words from the list \n",
    "dfn['review_t'] = dfn['review'].apply(lambda x: tokenizer_porter(x) )\n",
    "#dfn_tokenizer = [w for w in tokenizer_porter(dfn['review']) if w not in stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "927756f9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-18454ac2a3c2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdfn\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'review_t1'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdfn\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'review_t'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "dfn['review_t1'] = dfn['review_t'].apply(lambda x: w for w in x if w not in stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4c8838b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfn['review_t2'] = dfn['review_t1'].apply(lambda x: ' '.join([str(elem) for elem in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dd10a621",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1974 teenag martha moxley maggi grace move high class area bell greenwich connecticut mischief night eve halloween wa murder backyard hous murder remain unsolv twenti two year later writer mark fuhrman christoph meloni former la detect ha fallen disgrac perjuri j simpson trial move idaho decid investig case hi partner stephen week andrew mitchel purpos write book local squirm welcom support retir detect steve carrol robert forster wa charg investig 70 discov crimin net power money cover murder murder greenwich good tv movi true stori murder fifteen year old girl wa commit wealthi teenag whose mother wa kennedi power rich famili use influenc cover murder twenti year howev snoopi detect convict perjur disgrac wa abl disclos hideou crime wa commit screenplay show investig mark last day martha parallel lack emot dramat vote seven titl brazil avail'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfn['review_t2'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "58af7103",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tf=TfidfVectorizer()\n",
    "text_tf= tf.fit_transform(dfn['review_t2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b901b0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(\n",
    "    text_tf, dfn['sentiment'], test_size=0.50, random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ad2f1ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.88944\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "lr = LogisticRegression(C=10.0, penalty='l2', max_iter=1000, n_jobs=-1, random_state=12)\n",
    "\n",
    "lr.fit(X_train1, y_train1)\n",
    "predicted= lr.predict(X_test1)\n",
    "print(\"Logistic Regression Accuracy:\", metrics.accuracy_score(y_test1, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358aa2f8",
   "metadata": {},
   "source": [
    "## Serializing fitted scikit-learn estimators\n",
    "\n",
    "After we trained the logistic regression model as shown above, we know save the classifier along woth the stop words, Porter Stemmer, and HashingVectorizer as serialized objects to our local disk so that we can use the fitted classifier in our web application later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fe7ed703",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "#import dill as pickle\n",
    "import os\n",
    "\n",
    "dest = os.path.join('movieclassifier', 'pkl_objects')\n",
    "if not os.path.exists(dest):\n",
    "    os.makedirs(dest)\n",
    "\n",
    "#pickle.dump(stop, open(os.path.join(dest, 'stopwords.pkl'), 'wb'), protocol=4)   \n",
    "#pickle.dump(clf, open(os.path.join(dest, 'classifier.pkl'), 'wb'), protocol=4)\n",
    "#pickle.dump(stop, open(os.path.join(dest, 'stopwords.pkl'), 'wb'))   \n",
    "#pickle.dump(grid, open(os.path.join(dest, 'classifier.pkl'), 'wb'))\n",
    "pickle.dump(stop, open(os.path.join(dest, 'stopwords.pkl'), 'wb'), protocol=4)   \n",
    "pickle.dump(grid, open(os.path.join(dest, 'classifier.pkl'), 'wb'), protocol=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988c6d1a",
   "metadata": {},
   "source": [
    "Next, we save the HashingVectorizer as in a separate file so that we can import it later. Save it to `movieclassifier` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16d13b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile movieclassifier/vectorizer.py\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "import re\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "cur_dir = os.path.dirname(__file__)\n",
    "stop = pickle.load(open(\n",
    "                os.path.join(cur_dir, \n",
    "                'pkl_objects', \n",
    "                'stopwords.pkl'), 'rb'))\n",
    "\n",
    "def tokenizer(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)',\n",
    "                           text.lower())\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) \\\n",
    "                   + ' '.join(emoticons).replace('-', '')\n",
    "    tokenized = [w for w in text.split() if w not in stop]\n",
    "    return tokenized\n",
    "\n",
    "vect = HashingVectorizer(decode_error='ignore',\n",
    "                         n_features=2**21,\n",
    "                         preprocessor=None,\n",
    "                         tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1948741",
   "metadata": {},
   "source": [
    "`Overwriting movieclassifier/vectorizer.py`\n",
    "\n",
    "After executing the serialization code and creating `vectorizer.py` file in the `movieclassifier` folder, we can create new IPython notebook run from `movieclassifier` folder to check if the objects were serialized correctly.\n",
    "When you run new IPython notebook from `movieclassifier` folder then run the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107aa107",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "path = os.getcwd()\n",
    "\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f5f196",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('movieclassifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9771ae05",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()\n",
    "\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d2812aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pickle\n",
    "import dill as pickle\n",
    "import re\n",
    "import os\n",
    "#from vectorizer import vect\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "def tokenizer(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)',\n",
    "                           text.lower())\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) \\\n",
    "                   + ' '.join(emoticons).replace('-', '')\n",
    "    tokenized = [w for w in text.split() if w not in stop]\n",
    "    return tokenized\n",
    "\n",
    "vect = HashingVectorizer(decode_error='ignore',\n",
    "                         n_features=2**21,\n",
    "                         preprocessor=None,\n",
    "                         tokenizer=tokenizer)\n",
    "\n",
    "stop = pickle.load(open(os.path.join('movieclassifier\\pkl_objects', 'stopwords.pkl'), 'rb'))\n",
    "\n",
    "# Loading model to compare the results\n",
    "model = pickle.load(open(os.path.join('movieclassifier\\pkl_objects', 'classifier.pkl'), 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebad2d26",
   "metadata": {},
   "source": [
    "## Setting up a SQLite database for data storage\n",
    "\n",
    "\n",
    "Before you execute this code, please make sure that you are currently in the movieclassifier directory.\n",
    "\n",
    "Note that we are still on the \"movieclassifier\" subdirectory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f464a5ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\4471082\\\\Documents\\\\My Project\\\\Movieclassifier'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05dfca58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import os\n",
    "\n",
    "if os.path.exists('reviews.sqlite'):\n",
    "    os.remove('reviews.sqlite')\n",
    "\n",
    "conn = sqlite3.connect('reviews.sqlite')\n",
    "c = conn.cursor()\n",
    "c.execute('CREATE TABLE review_db (review TEXT, sentiment INTEGER, date TEXT)')\n",
    "\n",
    "example1 = 'I love this movie'\n",
    "c.execute(\"INSERT INTO review_db (review, sentiment, date) VALUES (?, ?, DATETIME('now'))\", (example1, 1))\n",
    "\n",
    "example2 = 'I disliked this movie'\n",
    "c.execute(\"INSERT INTO review_db (review, sentiment, date) VALUES (?, ?, DATETIME('now'))\", (example2, 0))\n",
    "\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077f0564",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('reviews.sqlite')\n",
    "c = conn.cursor()\n",
    "\n",
    "c.execute(\"SELECT * FROM review_db WHERE date BETWEEN '2017-01-01 10:10:10' AND DATETIME('now')\")\n",
    "results = c.fetchall()\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18688968",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c98c72d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
