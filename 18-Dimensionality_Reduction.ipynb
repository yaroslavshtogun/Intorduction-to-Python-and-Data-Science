{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction - Principal Component Analysis (PCA)\n",
    "\n",
    "1. Introduction to Principal Component Analysis\n",
    "2. Principal Component Analysis as dimensionality reduction\n",
    "3. Example of Principal Component Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to Principal Component Analysis (PCA)\n",
    "\n",
    "According to <i>Wikipedia</i>, PCA is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables (entities each of which takes on various numerical values) into a set of values of linearly uncorrelated variables called principal components.\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<b>But where all we can apply PCA?</b>\n",
    "\n",
    "<b>Data Visualization</b>: When working on any data related problem, the challenge in today's world is the sheer volume of data, and the variables/features that define that data. To solve a problem where data is the key, we need extensive data exploration like finding out how the variables are correlated or understanding the distribution of a few variables. Considering that there are a large number of variables or dimensions along which the data is distributed, visualization can be a challenge and almost impossible.\n",
    "\n",
    "Hence, PCA can do that for us since it projects the data into a lower dimension, thereby allowing us to visualize the data in a 2D or 3D space with a naked eye.\n",
    "\n",
    "<b>Speeding Machine Learning (ML) Algorithm</b>: Since PCA's main idea is dimensionality reduction, we can leverage that to speed up our machine learning algorithm's training and testing time considering our data has a lot of features, and the ML algorithm's learning is too slow.\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<b>What is a Principal Component?</b>\n",
    "\n",
    "Principal components are the key to PCA; they represent what's underneath the hood of our data. In a layman term, when the data is projected into a lower dimension (assume three dimensions) from a higher space, the three dimensions are nothing but the three Principal Components that captures (or holds) most of the variance (information) of our data.\n",
    "\n",
    "Principal components have both direction and magnitude. The direction represents across which principal axes the data is mostly spread out or has most variance and the magnitude signifies the amount of variance that Principal Component captures of the data when projected onto that axis. The principal components are a straight line, and the first principal component holds the most variance in the data. Each subsequent principal component is orthogonal to the last and has a lesser variance. In this way, given a set of x correlated variables over y samples we achieve a set of uncorrelated principal components over the same y samples.\n",
    "\n",
    "The reason we achieve uncorrelated principal components from the original features is that the correlated features contribute to the same principal component, thereby reducing the original data features into uncorrelated principal components; each representing a different set of correlated features with different amounts of variation.\n",
    "\n",
    "Each principal component represents a percentage of total variation captured from the data.\n",
    "\n",
    "The material is take from https://jakevdp.github.io/PythonDataScienceHandbook/05.09-principal-component-analysis.html\n",
    "and https://www.datacamp.com/community/tutorials/principal-component-analysis-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load labraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# Display all the columns of the dataframe\n",
    "pd.pandas.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal component analysis is a fast and flexible unsupervised method for dimensionality reduction in data. Its behavior is easiest to visualize by looking at a two-dimensional dataset. Consider the following 200 points plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot for 200 random points\n",
    "rng = np.random.RandomState(1)\n",
    "\n",
    "plt.figure(figsize = (5,5))  # Set Figure size \n",
    "\n",
    "X = np.dot(rng.rand(2, 2), rng.randn(2, 200)).T\n",
    "plt.scatter(X[:, 0], X[:, 1])\n",
    "plt.axis('equal')\n",
    "plt.xlim(-3,3)\n",
    "plt.ylim(-3,3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In principal component analysis, relationship between `x` and `y` is quantified by finding a list of the principal axes in the data, and using those axes to describe the dataset. Using Scikit-Learn's ```PCA``` estimator, we can compute this as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fit learns some quantities from the data, most importantly the \"components\" and \"explained variance\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca.explained_variance_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above output, we can observe that the `principal component 1` holds 76.3% of the information while the `principal component 2` holds only 1.8% of the information. Also, the other point to note is that while projecting to principal components, 21.9.0% information was lost.\n",
    "\n",
    "To see what these numbers mean, let's visualize them as vectors over the input data, using the \"components\" to define the direction of the vector, and the \"explained variance\" to define the squared-length of the vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_vector(v0, v1, ax=None):\n",
    "    ax = ax or plt.gca()\n",
    "    arrowprops=dict(arrowstyle='->',\n",
    "                    linewidth=2,\n",
    "                    shrinkA=0, shrinkB=0)\n",
    "    ax.annotate('', v1, v0, arrowprops=arrowprops)\n",
    "\n",
    "# plot data\n",
    "plt.figure(figsize = (5,5))  # Set Figure size \n",
    "plt.scatter(X[:, 0], X[:, 1], alpha=0.2)\n",
    "for length, vector in zip(pca.explained_variance_, pca.components_):\n",
    "    v = vector * 3 * np.sqrt(length)\n",
    "    draw_vector(pca.mean_, pca.mean_ + v)\n",
    "plt.axis('equal')\n",
    "plt.xlim(-3,3)\n",
    "plt.ylim(-3,3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These vectors represent the principal axes of the data, and the length of the vector is an indication of how \"important\" that axis is in describing the distribution of the dataâ€”more precisely, it is a measure of the variance of the data when projected onto that axis. The projection of each data point onto the principal axes are the \"principal components\" of the data.\n",
    "\n",
    "If we plot these principal components beside the original data, we see the plots shown here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(1)\n",
    "X = np.dot(rng.rand(2, 2), rng.randn(2, 200)).T\n",
    "pca = PCA(n_components=2, whiten=True)\n",
    "pca.fit(X)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n",
    "\n",
    "# plot data\n",
    "ax[0].scatter(X[:, 0], X[:, 1], alpha=0.2)\n",
    "for length, vector in zip(pca.explained_variance_, pca.components_):\n",
    "    v = vector * 3 * np.sqrt(length)\n",
    "    draw_vector(pca.mean_, pca.mean_ + v, ax=ax[0])\n",
    "ax[0].axis('equal');\n",
    "ax[0].set(xlabel='x', ylabel='y', title='input')\n",
    "\n",
    "# plot principal components\n",
    "X_pca = pca.transform(X)\n",
    "ax[1].scatter(X_pca[:, 0], X_pca[:, 1], alpha=0.2)\n",
    "draw_vector([0, 0], [0, 3], ax=ax[1])\n",
    "draw_vector([0, 0], [3, 0], ax=ax[1])\n",
    "ax[1].axis('equal')\n",
    "ax[1].set(xlabel='component 1', ylabel='component 2',\n",
    "          title='principal components',\n",
    "          xlim=(-5, 5), ylim=(-3, 3.1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Principal Component Analysis (PCA) as dimensionality reduction\n",
    "\n",
    "Using PCA for dimensionality reduction involves zeroing out one or more of the smallest principal components, resulting in a lower-dimensional projection of the data that preserves the maximal data variance.\n",
    "\n",
    "Here is an example of using PCA as a dimensionality reduction transform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=1)\n",
    "pca.fit(X)\n",
    "X_pca = pca.transform(X)\n",
    "print(\"original shape:   \", X.shape)\n",
    "print(\"transformed shape:\", X_pca.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformed data has been reduced to a single dimension. To understand the effect of this dimensionality reduction, we can perform the inverse transform of this reduced data and plot it along with the original data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = pca.inverse_transform(X_pca)\n",
    "plt.scatter(X[:, 0], X[:, 1], alpha=0.2)\n",
    "plt.scatter(X_new[:, 0], X_new[:, 1], alpha=0.8, color='blue')\n",
    "plt.xlim(-3,3)\n",
    "plt.ylim(-3,3)\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The light points are the original data, while the dark points are the projected version. This makes clear what a PCA dimensionality reduction means: the information along the least important principal axis or axes is removed, leaving only the component(s) of the data with the highest variance. The fraction of variance that is cut out (proportional to the spread of points about the line formed in this figure) is roughly a measure of how much \"information\" is discarded in this reduction of dimensionality.\n",
    "\n",
    "This reduced-dimension dataset is in some senses \"good enough\" to encode the most important relationships between the points: despite reducing the dimension of the data by 50%, the overall relationship between the data points are mostly preserved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Example of Principal Component Analysis (PCA)\n",
    "\n",
    "We are going to use `Breast Cancer dataset` from `Sklearn library`.\n",
    "\n",
    "The Breast Cancer data set is a real-valued multivariate data that consists of two classes, where each class signifies whether a patient has breast cancer or not. The two categories are: malignant and benign.\n",
    "\n",
    "The malignant class has 212 samples, whereas the benign class has 357 samples.\n",
    "\n",
    "It has 30 features shared across all classes: radius, texture, perimeter, area, smoothness, fractal dimension, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Breast Cancer Data Exploration\n",
    "from sklearn.datasets import load_breast_cancer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`load_breast_cancer` will give us both labels and the data. To fetch the data, we will call `.data` and for fetching the labels `.target`.\n",
    "\n",
    "The data has <b>569 samples</b> with <b>30 features</b>, and each sample has a label associated with it. There are two labels in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get both data and target\n",
    "breast = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data only\n",
    "breast_data = breast.data\n",
    "print('breast_data shape:',breast_data.shape)\n",
    "breast_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at target values even if we will not use it\n",
    "breast_labels = breast.target\n",
    "print('breast_labels shape:',breast_labels.shape)\n",
    "breast_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the feature names\n",
    "features = breast.feature_names\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame\n",
    "breast_dataset = pd.DataFrame(breast_data)\n",
    "breast_dataset.columns = features\n",
    "breast_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Label column\n",
    "breast_dataset['label'] = breast_labels.T # breast_labels transpose before adding to dataframe, to get it vertical\n",
    "breast_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the original labels are in 0,1 format, we will change the labels to benign and malignant using .replace function.\n",
    "# We will use inplace=True which will modify the dataframe breast_dataset.\n",
    "breast_dataset['label'].replace(0, 'Benign',inplace=True)\n",
    "breast_dataset['label'].replace(1, 'Malignant',inplace=True)\n",
    "breast_dataset.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now comes the most exciting part of this tutorial. As we learned earlier that PCA projects turn high-dimensional data into a low-dimensional principal component, now is the time to visualize that with the help of Python!\n",
    "\n",
    "To Visualizing the Breast Cancer data, we start by Standardizing the data since PCA's output is influenced based on the scale of the features of the data.\n",
    "\n",
    "It is a common practice to normalize our data before feeding it to any machine learning algorithm.\n",
    "\n",
    "To apply normalization, we will import `StandardScaler` module from the sklearn library and select only the features from the `breast_dataset` we created. Once we have the features, we will then apply scaling by doing `fit_transform` on the feature data.\n",
    "\n",
    "While applying `StandardScaler`, each feature of our data should be normally distributed such that it will scale the distribution to a mean of zero and a standard deviation of one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "x = breast_dataset.loc[:, features].values\n",
    "x = StandardScaler().fit_transform(x) # normalizing the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check whether the normalized data has a mean of zero and a standard deviation of one.\n",
    "np.mean(x),np.std(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's convert the normalized features into a tabular format with the help of DataFrame.\n",
    "feat_cols = ['feature' + str(i) for i in range(x.shape[1])]\n",
    "normalised_breast = pd.DataFrame(x,columns=feat_cols)\n",
    "normalised_breast.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now comes the critical part, the next few lines of code will be projecting the thirty-dimensional Breast Cancer data to two-dimensional `principal components`.\n",
    "\n",
    "We will use the sklearn library to import the `PCA` module, and in the PCA method, we will pass the number of components `(n_components=2)` and finally call fit_transform on the aggregate data. Here, several components represent the lower dimension in which we will project our higher dimension data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca_breast = PCA(n_components=2)\n",
    "principalComponents_breast = pca_breast.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, let's create a DataFrame that will have the principal component values for all 569 samples.\n",
    "principal_breast_Df = pd.DataFrame(data = principalComponents_breast\n",
    "             , columns = ['principal component 1', 'principal component 2'])\n",
    "principal_breast_Df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the principal components, we can find the `explained_variance_ratio`. It will provide us with the amount of information or variance each principal component holds after projecting the data to a lower dimensional subspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Explained variation per principal component: {}'.format(pca_breast.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above output, we can observe that the `principal component 1` holds 44.2% of the information while the `principal component 2` holds only 19% of the information. Also, the other point to note is that while projecting thirty-dimensional data to a two-dimensional data, 36.8% information was lost.\n",
    "\n",
    "Let's plot the visualization of the 569 samples along the `principal component - 1` and `principal component - 2` axis. It should give us good insight into how our samples are distributed among the two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.xlabel('Principal Component - 1',fontsize=20)\n",
    "plt.ylabel('Principal Component - 2',fontsize=20)\n",
    "plt.title(\"Principal Component Analysis of Breast Cancer Dataset\",fontsize=20)\n",
    "targets = ['Benign', 'Malignant']\n",
    "colors = ['r', 'g']\n",
    "for target, color in zip(targets,colors):\n",
    "    indicesToKeep = breast_dataset['label'] == target\n",
    "    plt.scatter(principal_breast_Df.loc[indicesToKeep, 'principal component 1']\n",
    "               , principal_breast_Df.loc[indicesToKeep, 'principal component 2'], c = color, s = 50)\n",
    "\n",
    "plt.legend(targets,prop={'size': 15})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above graph, we can observe that the two classes `benign` and `malignant`, when projected to a two-dimensional space, can be linearly separable up to some extent. Other observations can be that the `benign` class is spread out as compared to the `malignant` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
